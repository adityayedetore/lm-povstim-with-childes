{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Childes Preprocsssing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'childes-txt/excluded.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-0f50b09ea750>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mvalid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"childes-txt/valid.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"childes-txt/test.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mexcluded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"childes-txt/excluded.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mtreebank\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"childes-txt/treebank.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-0f50b09ea750>\u001b[0m in \u001b[0;36mread_data\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mread_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mraw_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlistify_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'childes-txt/excluded.txt'"
     ]
    }
   ],
   "source": [
    "def listify_data(raw_string):\n",
    "    return [line.split() for line in raw_string.splitlines()]\n",
    "\n",
    "def read_data(filename):\n",
    "    with open(filename, \"r\") as f:\n",
    "        raw_data = f.read()\n",
    "        data = listify_data(raw_data)\n",
    "    return data\n",
    "\n",
    "train = read_data(\"childes-txt/train.txt\")\n",
    "valid = read_data(\"childes-txt/valid.txt\")\n",
    "test = read_data(\"childes-txt/test.txt\")\n",
    "excluded = read_data(\"childes-txt/excluded.txt\")\n",
    "treebank = read_data(\"childes-txt/treebank.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unlistify_data(data):\n",
    "    zipped = [\" \".join(line) for line in data]\n",
    "    return \"\\n\".join(zipped)\n",
    "\n",
    "def write_data(data, filename):\n",
    "    with open(filename, \"w\") as f:\n",
    "        f.write(unlistify_data(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split possesives and contractions\n",
    "\n",
    "Seperate possesives: \"camel's\" -> \"camel 's\"\n",
    "\n",
    "Also seperate contractions. The subwords _n't_ , _'re_ , _'ll_ , _'m_ , _'ve_ , and _'d_ should be prepened with spaces. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this takes a minute and a half to run on my machine\n",
    "\n",
    "def split_possesives_and_contractions(word):\n",
    "    if word.endswith(\"'s\"):\n",
    "        return word[:-2] + \" 's\"\n",
    "    if word == \"can't\":\n",
    "        return \"can n't\"\n",
    "    if word.endswith(\"n't\"):\n",
    "        return word[:-3] + \" n't\"\n",
    "    if word.endswith(\"'re\"):\n",
    "        return word[:-3] + \" 're\"\n",
    "    if word.endswith(\"'m\"):\n",
    "        return word[:-2] + \" 'm\"\n",
    "    if word.endswith(\"'d\"):\n",
    "        return word[:-2] + \" 'd\"\n",
    "    if word.endswith(\"'ll\"):\n",
    "        return word[:-3] + \" 'll\"\n",
    "    if word.endswith(\"'ve\"):\n",
    "        return word[:-3] + \" 've\"\n",
    "    if word.endswith(\"s'\"):\n",
    "        return word[:-1] + \" 's'\"\n",
    "    if word.endswith(\"'r\"):\n",
    "        return word[:-2] + \" are\"\n",
    "    if word.endswith(\"'has\"):\n",
    "        return word[:-4] + \" has\"\n",
    "    if word.endswith(\"'is\"):\n",
    "        return word[:-3] + \" is\"\n",
    "    if word.endswith(\"'did\"):\n",
    "        return word[:-4] + \" did\"\n",
    "    if word == \"y'all\":\n",
    "        return \"you all\"\n",
    "    if word == \"c'mere\":\n",
    "        return \"come here\"\n",
    "    if word == \"I'ma\":\n",
    "        return \"I am going to\"\n",
    "    if word == \"what'cha\":\n",
    "        return \"what are you\"\n",
    "    if word == \"don'tcha\":\n",
    "        return \"do you not\"\n",
    "    \n",
    "    # List of startswith exceptions: [\"t'\", \"o'\", \"O'\", \"d'\"]\n",
    "    # List of == exceptions: [\"Ma'am\", \"ma'am\", \"An'\", \"b'ring\", \"Hawai'i\",\"don'ting\", \"rock'n'roll\" \"don'ting\", \"That'scop\",\"that'ss\",\"go'ed\", \"s'pose\", \"'hey\", \"me'\", \"shh'ell\", \"th'do\", \"Ross'a\", \"him'sed\"] \n",
    "    # List of in exceptions: [\"_\", \"-\"]\n",
    "    # List of endswith exceptions (note that this one is a catch all condition): [\"'\"]\n",
    "\n",
    "    return word\n",
    "\n",
    "def split_line(line):\n",
    "    s = [split_possesives_and_contractions(word) for word in line]\n",
    "    return \" \".join(s).split()\n",
    "\n",
    "def split_data(data):\n",
    "    return [split_line(line) for line in data]\n",
    "\n",
    "train_split = split_data(train)\n",
    "valid_split = split_data(valid)\n",
    "test_split = split_data(test)\n",
    "write_data(split_data(excluded), \"excluded-processed.txt\")\n",
    "write_data(split_data(excluded), \"treebank-processed.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unking\n",
    "\n",
    "Replace infrequent words with `<unk>` tokens. \n",
    "\n",
    "Note that the unked tokens are based on the training set, even for the validation and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_frequencies(data):\n",
    "    frequencies = {}\n",
    "    for line in data:\n",
    "        for word in line:\n",
    "            if word in frequencies:\n",
    "                frequencies[word] += 1\n",
    "            else:\n",
    "                frequencies[word] = 1\n",
    "    return frequencies\n",
    "\n",
    "# words with frequency > cutoff\n",
    "def make_vocab(data, cutoff):\n",
    "    frequencies = count_frequencies(data)\n",
    "    high_frequency_tokens = set()\n",
    "    for token in frequencies:\n",
    "        if frequencies[token] > cutoff:\n",
    "            high_frequency_tokens.add(token)\n",
    "    return high_frequency_tokens\n",
    "\n",
    "def unk(data, vocab):\n",
    "    unked_data = []\n",
    "    for line in data:\n",
    "        unked_line = []\n",
    "        for word in line:\n",
    "            if word in vocab:\n",
    "                unked_line.append(word)\n",
    "            else:\n",
    "                unked_line.append(\"<unk>\")\n",
    "        unked_data.append(unked_line) \n",
    "    return unked_data\n",
    "\n",
    "# train vocab used for train, valid, and test sets\n",
    "train_vocab = make_vocab(train_split, cutoff=2)\n",
    "train_unked = unk(train_split, train_vocab)\n",
    "valid_unked = unk(valid_split, train_vocab)\n",
    "test_unked  = unk(test_split,  train_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open (\"childes-txt-unked/vocab.txt\", 'w') as f:\n",
    "    f.write(\"\\n\".join(train_vocab))\n",
    "write_data(train_unked, \"childes-txt-unked/train.txt\")\n",
    "write_data(valid_unked, \"childes-txt-unked/valid.txt\")\n",
    "write_data(test_unked, \"childes-txt-unked/test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
